{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919aee20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 images belonging to 2 classes.\n",
      "Found 16 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n",
      "Computed Class Weights: {0: np.float64(1.9448173005219984), 1: np.float64(0.6730322580645162)}\n",
      "Training Autoencoder...\n",
      "Found 5216 images belonging to 2 classes.\n",
      "Found 16 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m348s\u001b[0m 2s/step - loss: 0.0069 - val_loss: 0.0018\n",
      "Epoch 2/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m312s\u001b[0m 2s/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 3/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m343s\u001b[0m 2s/step - loss: 8.2279e-04 - val_loss: 0.0013\n",
      "Epoch 4/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 2s/step - loss: 6.4812e-04 - val_loss: 0.0013\n",
      "Epoch 5/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 2s/step - loss: 5.9730e-04 - val_loss: 0.0012\n",
      "Epoch 6/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 2s/step - loss: 5.4644e-04 - val_loss: 0.0011\n",
      "Epoch 7/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m301s\u001b[0m 2s/step - loss: 4.8559e-04 - val_loss: 0.0010\n",
      "Epoch 8/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m247s\u001b[0m 2s/step - loss: 4.6139e-04 - val_loss: 0.0011\n",
      "Epoch 9/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 1s/step - loss: 4.2903e-04 - val_loss: 9.6699e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 1s/step - loss: 4.0240e-04 - val_loss: 8.7591e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder & Encoder Saved!\n",
      "Reconstructing datasets...\n",
      "Train: (5216, 224, 224, 3) Val: (16, 224, 224, 3) Test: (624, 224, 224, 3)\n",
      "Training classifier...\n",
      "Epoch 1/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 745ms/step - accuracy: 0.7184 - loss: 1.0394 - val_accuracy: 0.8750 - val_loss: 0.4983 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 744ms/step - accuracy: 0.7893 - loss: 0.4208 - val_accuracy: 0.6250 - val_loss: 0.5432 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 731ms/step - accuracy: 0.7345 - loss: 0.4260 - val_accuracy: 0.8750 - val_loss: 0.4334 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 729ms/step - accuracy: 0.7044 - loss: 0.4409 - val_accuracy: 0.8125 - val_loss: 0.4566 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 725ms/step - accuracy: 0.6716 - loss: 0.4338 - val_accuracy: 0.9375 - val_loss: 0.4220 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 724ms/step - accuracy: 0.7182 - loss: 0.3983 - val_accuracy: 0.8125 - val_loss: 0.4864 - learning_rate: 0.0010\n",
      "Epoch 7/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 725ms/step - accuracy: 0.7234 - loss: 0.4013 - val_accuracy: 0.8125 - val_loss: 0.4619 - learning_rate: 0.0010\n",
      "Epoch 8/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 727ms/step - accuracy: 0.6545 - loss: 0.4154 - val_accuracy: 0.8750 - val_loss: 0.3742 - learning_rate: 0.0010\n",
      "Epoch 9/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 726ms/step - accuracy: 0.5934 - loss: 0.4563 - val_accuracy: 0.8750 - val_loss: 0.3774 - learning_rate: 0.0010\n",
      "Epoch 10/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 735ms/step - accuracy: 0.5962 - loss: 0.4327 - val_accuracy: 0.7500 - val_loss: 0.4547 - learning_rate: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 1. Imports\n",
    "# =========================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input, Conv2D, MaxPooling2D, UpSampling2D, Flatten\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# =========================\n",
    "# 2. Data Directories & Params\n",
    "# =========================\n",
    "train_dir = '../data/train'\n",
    "val_dir = '../data/val'\n",
    "test_dir = '../data/test'\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# =========================\n",
    "# 3. Data Generators\n",
    "# =========================\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE,\n",
    "    class_mode='binary', shuffle=True\n",
    ")\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    val_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE,\n",
    "    class_mode='binary', shuffle=False\n",
    ")\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE,\n",
    "    class_mode='binary', shuffle=False\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 4. Class Weights\n",
    "# =========================\n",
    "cw = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_generator.classes),\n",
    "    y=train_generator.classes\n",
    ")\n",
    "class_weights = dict(enumerate(cw))\n",
    "print(\"Computed Class Weights:\", class_weights)\n",
    "\n",
    "# =========================\n",
    "# 5. Autoencoder Definition\n",
    "# =========================\n",
    "input_img = Input(shape=(224, 224, 3))\n",
    "# Encoder\n",
    "x = Conv2D(32, (3,3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2,2), padding='same')(x)\n",
    "x = Conv2D(64, (3,3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2,2), padding='same')(x)\n",
    "# Decoder\n",
    "x = Conv2D(64, (3,3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2,2))(x)\n",
    "x = Conv2D(32, (3,3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2,2))(x)\n",
    "decoded = Conv2D(3, (3,3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "encoder = Model(input_img, encoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# =========================\n",
    "# 6. Autoencoder Training\n",
    "# =========================\n",
    "autoencoder_path = \"autoencoder_model.h5\"\n",
    "encoder_path = \"encoder_model.h5\"\n",
    "\n",
    "if os.path.exists(autoencoder_path) and os.path.exists(encoder_path):\n",
    "    print(\"Loading pre-trained Autoencoder and Encoder...\")\n",
    "    autoencoder = load_model(autoencoder_path)\n",
    "    encoder = load_model(encoder_path)\n",
    "else:\n",
    "    print(\"Training Autoencoder...\")\n",
    "    # Train autoencoder with image generators, only images as input\n",
    "    ae_train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode=None, shuffle=True)\n",
    "    ae_val_generator = val_datagen.flow_from_directory(\n",
    "        val_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode=None, shuffle=False)\n",
    "\n",
    "    # Custom generator for autoencoder fitting\n",
    "    def autoencoder_gen(gen):\n",
    "        for batch in gen:            yield (batch, batch)\n",
    "    steps_per_epoch = len(ae_train_generator)\n",
    "    val_steps = len(ae_val_generator)\n",
    "    autoencoder.fit(\n",
    "        autoencoder_gen(ae_train_generator), \n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=10,\n",
    "        validation_data=autoencoder_gen(ae_val_generator),\n",
    "        validation_steps=val_steps\n",
    "    )\n",
    "    autoencoder.save(autoencoder_path)\n",
    "    encoder.save(encoder_path)\n",
    "    print(\"Autoencoder & Encoder Saved!\")\n",
    "\n",
    "# =========================\n",
    "# 7. Reconstruct Datasets Using Autoencoder\n",
    "# =========================\n",
    "def build_reconstructed_dataset(labeled_gen, model_ae):\n",
    "    x_list, y_list = [], []\n",
    "    labeled_gen.reset()\n",
    "    for _ in range(len(labeled_gen)):\n",
    "        x_batch, y_batch = next(labeled_gen)\n",
    "        x_recon = model_ae.predict(x_batch, verbose=0)\n",
    "        x_list.append(x_recon)\n",
    "        y_list.append(y_batch)\n",
    "    return np.concatenate(x_list), np.concatenate(y_list)\n",
    "print(\"Reconstructing datasets...\")\n",
    "X_train_recon, y_train = build_reconstructed_dataset(train_generator, autoencoder)\n",
    "X_val_recon, y_val = build_reconstructed_dataset(validation_generator, autoencoder)\n",
    "X_test_recon, y_test = build_reconstructed_dataset(test_generator, autoencoder)\n",
    "print(\"Train:\", X_train_recon.shape, \"Val:\", X_val_recon.shape, \"Test:\", X_test_recon.shape)\n",
    "\n",
    "# =========================\n",
    "# 8. Classifier on Top of Encoder\n",
    "# =========================\n",
    "encoder.trainable = False # freeze encoder\n",
    "\n",
    "x = Flatten()(encoder.output) # crucial: flatten output for Dense layers\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "classifier_output = Dense(1, activation='sigmoid')(x)\n",
    "classifier = Model(encoder.input, classifier_output, name='classifier')\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=3, verbose=1)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# =========================\n",
    "# 9. Train the Classifier\n",
    "# =========================\n",
    "print(\"Training classifier...\")\n",
    "history = classifier.fit(\n",
    "    ''\n",
    "    X_train_recon, y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=10,\n",
    "    validation_data=(X_val_recon, y_val),\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[lr_scheduler, early_stop]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e125d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ResNet50 on autoencoder-reconstructed images...\n",
      "Epoch 1/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m274s\u001b[0m 2s/step - accuracy: 0.4965 - loss: 0.7571 - val_accuracy: 0.5625 - val_loss: 0.6779 - learning_rate: 1.0000e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 2s/step - accuracy: 0.5656 - loss: 0.6771 - val_accuracy: 0.5625 - val_loss: 0.6623 - learning_rate: 1.0000e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5838 - loss: 0.6623\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 2.9999999242136255e-05.\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 2s/step - accuracy: 0.6192 - loss: 0.6472 - val_accuracy: 0.5625 - val_loss: 0.6775 - learning_rate: 1.0000e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 2s/step - accuracy: 0.6823 - loss: 0.6336 - val_accuracy: 0.6250 - val_loss: 0.6430 - learning_rate: 3.0000e-05\n",
      "Epoch 5/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m278s\u001b[0m 2s/step - accuracy: 0.7147 - loss: 0.6234 - val_accuracy: 0.8125 - val_loss: 0.6399 - learning_rate: 3.0000e-05\n",
      "Fine-tuning ResNet50...\n",
      "Epoch 1/5\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1694s\u001b[0m 10s/step - accuracy: 0.8163 - loss: 0.3408 - val_accuracy: 0.5000 - val_loss: 0.9503 - learning_rate: 1.0000e-05\n",
      "Epoch 2/5\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1412s\u001b[0m 9s/step - accuracy: 0.9559 - loss: 0.1148 - val_accuracy: 0.5000 - val_loss: 0.7677 - learning_rate: 1.0000e-05\n",
      "Epoch 3/5\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - accuracy: 0.9812 - loss: 0.0609\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 2.9999999242136253e-06.\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1245s\u001b[0m 8s/step - accuracy: 0.9816 - loss: 0.0580 - val_accuracy: 0.4375 - val_loss: 1.0481 - learning_rate: 1.0000e-05\n",
      "Epoch 4/5\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1247s\u001b[0m 8s/step - accuracy: 0.9912 - loss: 0.0338 - val_accuracy: 0.5000 - val_loss: 1.6339 - learning_rate: 3.0000e-06\n",
      "Epoch 5/5\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1245s\u001b[0m 8s/step - accuracy: 0.9948 - loss: 0.0237 - val_accuracy: 0.5000 - val_loss: 2.4966 - learning_rate: 3.0000e-06\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.3750 - loss: 1.1555\n",
      "ResNet50 Test Loss: 1.1555, Test Accuracy: 0.3750\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load ResNet50 base without top layers, freeze initially\n",
    "resnet_base = ResNet50(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "resnet_base.trainable = False\n",
    "\n",
    "# Build ResNet50 model\n",
    "resnet_input = Input(shape=(224,224,3))\n",
    "x = resnet_base(resnet_input, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "resnet_output = Dense(1, activation='sigmoid')(x)\n",
    "resnet_model = Model(resnet_input, resnet_output)\n",
    "\n",
    "# Compile\n",
    "resnet_model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train on reconstructed images\n",
    "print(\"Training ResNet50 on autoencoder-reconstructed images...\")\n",
    "history_resnet = resnet_model.fit(\n",
    "    X_train_recon, y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=10,\n",
    "    validation_data=(X_val_recon, y_val),\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[lr_scheduler, early_stop]\n",
    ")\n",
    "\n",
    "# Optional: fine-tune whole ResNet50 with lower LR\n",
    "resnet_base.trainable = True\n",
    "resnet_model.compile(optimizer=Adam(1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(\"Fine-tuning ResNet50...\")\n",
    "history_resnet_ft = resnet_model.fit(\n",
    "    X_train_recon, y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=5,\n",
    "    validation_data=(X_val_recon, y_val),\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[lr_scheduler, early_stop]\n",
    ")\n",
    "\n",
    "# Evaluate on reconstructed test set\n",
    "test_loss, test_acc = resnet_model.evaluate(X_test_recon, y_test, batch_size=BATCH_SIZE)\n",
    "print(f\"ResNet50 Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d7ece9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
