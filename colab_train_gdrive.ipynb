{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü´Å Pneumonia Detection Training - Google Drive Version\n",
    "\n",
    "This notebook trains your pneumonia detection models using your dataset from Google Drive.\n",
    "\n",
    "**Advantages of Google Drive approach:**\n",
    "- No need to upload large files each time\n",
    "- Faster access to your dataset\n",
    "- Can reuse the same dataset across multiple sessions\n",
    "- More reliable for large datasets\n",
    "\n",
    "**Prerequisites:**\n",
    "1. Upload your `chest_xray` folder to Google Drive\n",
    "2. Note the path where you uploaded it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Step 1: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"CUDA available:\", tf.test.is_built_with_cuda())\n",
    "\n",
    "# Enable GPU memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"‚úÖ GPU memory growth enabled\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU found, using CPU (training will be slower)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install seaborn opencv-python-headless\n",
    "\n",
    "# Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Activation, Concatenate\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import zipfile\n",
    "from google.colab import files, drive\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "print(\"‚úÖ All packages imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ Step 2: Mount Google Drive and Setup Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "print(\"üìÅ Mounting Google Drive...\")\n",
    "drive.mount('/content/drive')\n",
    "print(\"‚úÖ Google Drive mounted successfully!\")\n",
    "\n",
    "# List contents of Google Drive to help you find your dataset\n",
    "print(\"\\nüìÇ Contents of your Google Drive:\")\n",
    "drive_root = '/content/drive/MyDrive'\n",
    "if os.path.exists(drive_root):\n",
    "    for item in sorted(os.listdir(drive_root)):\n",
    "        item_path = os.path.join(drive_root, item)\n",
    "        item_type = \"üìÅ\" if os.path.isdir(item_path) else \"üìÑ\"\n",
    "        print(f\"  {item_type} {item}\")\n",
    "else:\n",
    "    print(\"‚ùå Could not access Google Drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure your dataset path in Google Drive\n",
    "# MODIFY THIS PATH to match where you uploaded your chest_xray folder\n",
    "\n",
    "# Common paths (uncomment the one that matches your setup):\n",
    "GDRIVE_DATASET_PATH = '/content/drive/MyDrive/chest_xray'  # If you uploaded chest_xray directly to Drive root\n",
    "# GDRIVE_DATASET_PATH = '/content/drive/MyDrive/pneumonia_project/chest_xray'  # If inside a project folder\n",
    "# GDRIVE_DATASET_PATH = '/content/drive/MyDrive/data/chest_xray'  # If inside a data folder\n",
    "\n",
    "print(f\"Looking for dataset at: {GDRIVE_DATASET_PATH}\")\n",
    "\n",
    "# Verify the dataset exists\n",
    "if os.path.exists(GDRIVE_DATASET_PATH):\n",
    "    print(\"‚úÖ Dataset found in Google Drive!\")\n",
    "    \n",
    "    # Check subdirectories\n",
    "    subdirs = ['train', 'val', 'test']\n",
    "    for subdir in subdirs:\n",
    "        subdir_path = os.path.join(GDRIVE_DATASET_PATH, subdir)\n",
    "        if os.path.exists(subdir_path):\n",
    "            print(f\"  ‚úÖ {subdir} directory found\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå {subdir} directory missing\")\nelse:\n",
    "    print(\"‚ùå Dataset not found at the specified path!\")\n",
    "    print(\"\\nüîß To fix this:\")\n",
    "    print(\"1. Make sure you've uploaded your chest_xray folder to Google Drive\")\n",
    "    print(\"2. Update the GDRIVE_DATASET_PATH variable above to match your folder location\")\n",
    "    print(\"3. Re-run this cell\")\n",
    "    \n",
    "    # Show available directories to help user find the right path\n",
    "    print(\"\\nüìÇ Available directories in your Drive:\")\n",
    "    for item in os.listdir(drive_root):\n",
    "        item_path = os.path.join(drive_root, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            print(f\"  üìÅ {item}\")\n",
    "            # Check if this directory contains chest_xray\n",
    "            chest_xray_path = os.path.join(item_path, 'chest_xray')\n",
    "            if os.path.exists(chest_xray_path):\n",
    "                print(f\"    üí° Found chest_xray in: /content/drive/MyDrive/{item}/chest_xray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy dataset from Google Drive to local Colab storage for faster access\n",
    "# This is optional but recommended for better performance\n",
    "\n",
    "COPY_TO_LOCAL = True  # Set to False if you want to use Drive directly (slower but saves space)\n",
    "\n",
    "if COPY_TO_LOCAL and os.path.exists(GDRIVE_DATASET_PATH):\n",
    "    print(\"üìã Copying dataset from Google Drive to local Colab storage...\")\n",
    "    print(\"This will take a few minutes but will make training faster.\")\n",
    "    \n",
    "    LOCAL_DATASET_PATH = '/content/chest_xray'\n",
    "    \n",
    "    # Remove existing local copy if it exists\n",
    "    if os.path.exists(LOCAL_DATASET_PATH):\n",
    "        shutil.rmtree(LOCAL_DATASET_PATH)\n",
    "        print(\"üóëÔ∏è Removed existing local copy\")\n",
    "    \n",
    "    # Copy from Drive to local\n",
    "    start_time = time.time()\n",
    "    shutil.copytree(GDRIVE_DATASET_PATH, LOCAL_DATASET_PATH)\n",
    "    copy_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"‚úÖ Dataset copied to local storage in {copy_time:.1f} seconds\")\n",
    "    DATASET_PATH = LOCAL_DATASET_PATH\n",
    "    \n",
    "else:\n",
    "    print(\"üìÅ Using dataset directly from Google Drive\")\n",
    "    DATASET_PATH = GDRIVE_DATASET_PATH\n",
    "\n",
    "print(f\"\\nüéØ Using dataset path: {DATASET_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset structure and count images\n",
    "def analyze_dataset(dataset_path):\n",
    "    \"\"\"Analyze and display dataset structure\"\"\"\n",
    "    if not os.path.exists(dataset_path):\n",
    "        print(f\"‚ùå Dataset not found at: {dataset_path}\")\n",
    "        return False\n",
    "    \n",
    "    splits = ['train', 'val', 'test']\n",
    "    classes = ['NORMAL', 'PNEUMONIA']\n",
    "    total_images = 0\n",
    "    \n",
    "    print(f\"\\nüìä Dataset Analysis: {dataset_path}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for split in splits:\n",
    "        split_path = os.path.join(dataset_path, split)\n",
    "        if os.path.exists(split_path):\n",
    "            normal_path = os.path.join(split_path, 'NORMAL')\n",
    "            pneumonia_path = os.path.join(split_path, 'PNEUMONIA')\n",
    "            \n",
    "            normal_count = 0\n",
    "            pneumonia_count = 0\n",
    "            \n",
    "            if os.path.exists(normal_path):\n",
    "                normal_files = [f for f in os.listdir(normal_path) \n",
    "                              if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "                normal_count = len(normal_files)\n",
    "            \n",
    "            if os.path.exists(pneumonia_path):\n",
    "                pneumonia_files = [f for f in os.listdir(pneumonia_path) \n",
    "                                 if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "                pneumonia_count = len(pneumonia_files)\n",
    "            \n",
    "            split_total = normal_count + pneumonia_count\n",
    "            total_images += split_total\n",
    "            \n",
    "            print(f\"{split.upper():>10}: Normal={normal_count:>4}, Pneumonia={pneumonia_count:>4}, Total={split_total:>4}\")\n",
    "            \n",
    "            if split_total > 0:\n",
    "                normal_pct = (normal_count / split_total) * 100\n",
    "                pneumonia_pct = (pneumonia_count / split_total) * 100\n",
    "                print(f\"{'':>10}  Distribution: Normal {normal_pct:.1f}% | Pneumonia {pneumonia_pct:.1f}%\")\n",
    "        else:\n",
    "            print(f\"{split.upper():>10}: ‚ùå Directory not found\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(f\"üìä TOTAL IMAGES: {total_images:,}\")\n",
    "    \n",
    "    # Show sample file names\n",
    "    sample_path = os.path.join(dataset_path, 'train', 'NORMAL')\n",
    "    if os.path.exists(sample_path):\n",
    "        sample_files = os.listdir(sample_path)[:3]\n",
    "        print(f\"\\nüìÑ Sample files: {', '.join(sample_files)}\")\n",
    "    \n",
    "    return total_images > 0\n",
    "\n",
    "# Analyze the dataset\n",
    "dataset_ready = analyze_dataset(DATASET_PATH)\n",
    "\n",
    "if dataset_ready:\n",
    "    print(\"\\n‚úÖ Dataset is ready for training!\")\nelse:\n",
    "    print(\"\\n‚ùå Dataset setup failed. Please check your Google Drive upload.\")\n",
    "    raise Exception(\"Dataset not ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 3: Configuration and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab-optimized configuration\n",
    "class ColabConfig:\n",
    "    # Data paths - use the dataset path we found\n",
    "    DATA_DIR = DATASET_PATH\n",
    "    TRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "    VAL_DIR = os.path.join(DATA_DIR, \"val\")\n",
    "    TEST_DIR = os.path.join(DATA_DIR, \"test\")\n",
    "    \n",
    "    # Model paths\n",
    "    MODEL_DIR = \"models\"\n",
    "    AUTOENCODER_PATH = os.path.join(MODEL_DIR, \"autoencoder_colab.h5\")\n",
    "    CLASSIFIER_PATH = os.path.join(MODEL_DIR, \"resnet_classifier_colab.h5\")\n",
    "    HYBRID_MODEL_PATH = os.path.join(MODEL_DIR, \"hybrid_model_colab.h5\")\n",
    "    \n",
    "    # Image parameters - Optimized for Colab GPU\n",
    "    IMG_HEIGHT = 224\n",
    "    IMG_WIDTH = 224\n",
    "    IMG_CHANNELS = 3\n",
    "    \n",
    "    # Training parameters - GPU optimized\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS_AUTOENCODER = 20\n",
    "    EPOCHS_CLASSIFIER = 15\n",
    "    LEARNING_RATE = 0.001\n",
    "    \n",
    "    # Classes\n",
    "    CLASSES = ['NORMAL', 'PNEUMONIA']\n",
    "    NUM_CLASSES = len(CLASSES)\n",
    "\n",
    "config = ColabConfig()\n",
    "os.makedirs(config.MODEL_DIR, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Configuration set up!\")\n",
    "print(f\"Dataset directory: {config.DATA_DIR}\")\n",
    "print(f\"Models will be saved to: {config.MODEL_DIR}\")\n",
    "print(f\"Image size: {config.IMG_HEIGHT}x{config.IMG_WIDTH}\")\n",
    "print(f\"Batch size: {config.BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generators\n",
    "def create_data_generators():\n",
    "    print(\"Creating data generators...\")\n",
    "    \n",
    "    # Training data generator with augmentation\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        zoom_range=0.2,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    \n",
    "    # Validation and test data generators\n",
    "    val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        config.TRAIN_DIR,\n",
    "        target_size=(config.IMG_HEIGHT, config.IMG_WIDTH),\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        classes=config.CLASSES\n",
    "    )\n",
    "    \n",
    "    val_generator = val_test_datagen.flow_from_directory(\n",
    "        config.VAL_DIR,\n",
    "        target_size=(config.IMG_HEIGHT, config.IMG_WIDTH),\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        classes=config.CLASSES\n",
    "    )\n",
    "    \n",
    "    test_generator = val_test_datagen.flow_from_directory(\n",
    "        config.TEST_DIR,\n",
    "        target_size=(config.IMG_HEIGHT, config.IMG_WIDTH),\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        classes=config.CLASSES,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    return train_generator, val_generator, test_generator\n",
    "\n",
    "def create_autoencoder_generators():\n",
    "    print(\"Creating autoencoder data generators...\")\n",
    "    \n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=10,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        horizontal_flip=True\n",
    "    )\n",
    "    \n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        config.TRAIN_DIR,\n",
    "        target_size=(config.IMG_HEIGHT, config.IMG_WIDTH),\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        class_mode='input'\n",
    "    )\n",
    "    \n",
    "    val_generator = val_datagen.flow_from_directory(\n",
    "        config.VAL_DIR,\n",
    "        target_size=(config.IMG_HEIGHT, config.IMG_WIDTH),\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        class_mode='input'\n",
    "    )\n",
    "    \n",
    "    return train_generator, val_generator\n",
    "\n",
    "# Test data generators\n",
    "try:\n",
    "    train_gen_test, val_gen_test, test_gen_test = create_data_generators()\n",
    "    print(\"‚úÖ Data generators created successfully!\")\n",
    "    print(f\"Training samples: {train_gen_test.samples:,}\")\n",
    "    print(f\"Validation samples: {val_gen_test.samples:,}\")\n",
    "    print(f\"Test samples: {test_gen_test.samples:,}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating data generators: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Step 4: Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder model\n",
    "def build_autoencoder():\n",
    "    input_img = Input(shape=(config.IMG_HEIGHT, config.IMG_WIDTH, config.IMG_CHANNELS))\n",
    "    \n",
    "    # Encoder\n",
    "    x = Conv2D(64, (3, 3), padding='same')(input_img)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    \n",
    "    x = Conv2D(256, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    \n",
    "    x = Conv2D(512, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    \n",
    "    # Decoder\n",
    "    x = Conv2D(512, (3, 3), padding='same')(encoded)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(256, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    \n",
    "    decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "    \n",
    "    autoencoder = Model(input_img, decoded)\n",
    "    encoder = Model(input_img, encoded)\n",
    "    \n",
    "    return autoencoder, encoder\n",
    "\n",
    "# ResNet50 classifier\n",
    "def build_resnet_classifier():\n",
    "    input_layer = Input(shape=(config.IMG_HEIGHT, config.IMG_WIDTH, config.IMG_CHANNELS))\n",
    "    \n",
    "    base_model = ResNet50(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_tensor=input_layer\n",
    "    )\n",
    "    \n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    predictions = Dense(config.NUM_CLASSES, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "# Hybrid model\n",
    "def build_hybrid_model(encoder_model):\n",
    "    input_layer = Input(shape=(config.IMG_HEIGHT, config.IMG_WIDTH, config.IMG_CHANNELS))\n",
    "    \n",
    "    # Freeze encoder\n",
    "    for layer in encoder_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    encoder_features = encoder_model(input_layer)\n",
    "    encoder_features_flat = GlobalAveragePooling2D()(encoder_features)\n",
    "    \n",
    "    # ResNet50 branch\n",
    "    resnet_base = ResNet50(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_tensor=input_layer\n",
    "    )\n",
    "    \n",
    "    # Freeze early ResNet layers\n",
    "    for layer in resnet_base.layers[:-10]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    resnet_features = resnet_base.output\n",
    "    resnet_features_flat = GlobalAveragePooling2D()(resnet_features)\n",
    "    \n",
    "    # Combine features\n",
    "    combined_features = Concatenate()([encoder_features_flat, resnet_features_flat])\n",
    "    \n",
    "    # Classification head\n",
    "    x = Dense(1024, activation='relu')(combined_features)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    predictions = Dense(config.NUM_CLASSES, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=predictions)\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"‚úÖ All model architectures defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Step 5: Train Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß TRAINING AUTOENCODER\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Build and compile autoencoder\n",
    "autoencoder, encoder = build_autoencoder()\n",
    "autoencoder.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=config.LEARNING_RATE),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "print(\"Autoencoder Summary:\")\n",
    "autoencoder.summary()\n",
    "\n",
    "# Create data generators\n",
    "train_gen_ae, val_gen_ae = create_autoencoder_generators()\n",
    "\n",
    "# Callbacks\n",
    "callbacks_ae = [\n",
    "    ModelCheckpoint(\n",
    "        config.AUTOENCODER_PATH,\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train autoencoder\n",
    "start_time = time.time()\n",
    "\n",
    "history_ae = autoencoder.fit(\n",
    "    train_gen_ae,\n",
    "    epochs=config.EPOCHS_AUTOENCODER,\n",
    "    validation_data=val_gen_ae,\n",
    "    callbacks=callbacks_ae,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "ae_time = time.time() - start_time\n",
    "print(f\"‚úÖ Autoencoder training completed in {ae_time/60:.1f} minutes\")\n",
    "\n",
    "# Save encoder separately\n",
    "encoder_path = os.path.join(config.MODEL_DIR, \"encoder_colab.h5\")\n",
    "encoder.save(encoder_path)\n",
    "print(f\"‚úÖ Encoder saved to {encoder_path}\")\n",
    "\n",
    "# Also save to Google Drive as backup\n",
    "gdrive_models_dir = '/content/drive/MyDrive/pneumonia_models'\n",
    "os.makedirs(gdrive_models_dir, exist_ok=True)\n",
    "shutil.copy2(config.AUTOENCODER_PATH, gdrive_models_dir)\n",
    "shutil.copy2(encoder_path, gdrive_models_dir)\n",
    "print(f\"‚úÖ Models backed up to Google Drive: {gdrive_models_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Step 6: Train ResNet50 Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß† TRAINING RESNET50 CLASSIFIER\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Build classifier\n",
    "classifier, base_model = build_resnet_classifier()\n",
    "\n",
    "# Create data generators\n",
    "train_gen, val_gen, test_gen = create_data_generators()\n",
    "\n",
    "# Phase 1: Frozen base layers\n",
    "print(\"Phase 1: Training with frozen base layers...\")\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "classifier.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=config.LEARNING_RATE),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', 'precision', 'recall']\n",
    ")\n",
    "\n",
    "callbacks_phase1 = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "history_phase1 = classifier.fit(\n",
    "    train_gen,\n",
    "    epochs=5,\n",
    "    validation_data=val_gen,\n",
    "    callbacks=callbacks_phase1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Phase 2: Fine-tuning\n",
    "print(\"Phase 2: Fine-tuning with unfrozen layers...\")\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "classifier.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=config.LEARNING_RATE/10),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', 'precision', 'recall']\n",
    ")\n",
    "\n",
    "callbacks_phase2 = [\n",
    "    ModelCheckpoint(\n",
    "        config.CLASSIFIER_PATH,\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.3,\n",
    "        patience=3,\n",
    "        min_lr=1e-8,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history_phase2 = classifier.fit(\n",
    "    train_gen,\n",
    "    epochs=config.EPOCHS_CLASSIFIER,\n",
    "    validation_data=val_gen,\n",
    "    callbacks=callbacks_phase2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "classifier_time = time.time() - start_time\n",
    "print(f\"‚úÖ Classifier training completed in {classifier_time/60:.1f} minutes\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"Evaluating classifier on test set...\")\n",
    "test_loss, test_acc, test_precision, test_recall = classifier.evaluate(test_gen, verbose=1)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n",
    "\n",
    "# Backup to Google Drive\n",
    "shutil.copy2(config.CLASSIFIER_PATH, gdrive_models_dir)\n",
    "print(f\"‚úÖ Classifier backed up to Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÄ Step 7: Train Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÄ TRAINING HYBRID MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load pre-trained encoder\n",
    "encoder = tf.keras.models.load_model(encoder_path)\n",
    "\n",
    "# Build hybrid model\n",
    "hybrid_model = build_hybrid_model(encoder)\n",
    "\n",
    "hybrid_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=config.LEARNING_RATE),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', 'precision', 'recall']\n",
    ")\n",
    "\n",
    "print(\"Hybrid Model Summary:\")\n",
    "hybrid_model.summary()\n",
    "\n",
    "# Callbacks\n",
    "callbacks_hybrid = [\n",
    "    ModelCheckpoint(\n",
    "        config.HYBRID_MODEL_PATH,\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history_hybrid = hybrid_model.fit(\n",
    "    train_gen,\n",
    "    epochs=config.EPOCHS_CLASSIFIER,\n",
    "    validation_data=val_gen,\n",
    "    callbacks=callbacks_hybrid,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "hybrid_time = time.time() - start_time\n",
    "print(f\"‚úÖ Hybrid model training completed in {hybrid_time/60:.1f} minutes\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"Evaluating hybrid model on test set...\")\n",
    "test_loss, test_acc, test_precision, test_recall = hybrid_model.evaluate(test_gen, verbose=1)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n",
    "\n",
    "# Backup to Google Drive\n",
    "shutil.copy2(config.HYBRID_MODEL_PATH, gdrive_models_dir)\n",
    "print(f\"‚úÖ Hybrid model backed up to Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 8: Training Results and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training histories\n",
    "def plot_training_history(history, title):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Accuracy\n",
    "    if 'accuracy' in history.history:\n",
    "        axes[0, 0].plot(history.history['accuracy'], label='Training Accuracy')\n",
    "        axes[0, 0].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        axes[0, 0].set_title(f'{title} - Accuracy')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Accuracy')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True)\n",
    "    \n",
    "    # Loss\n",
    "    axes[0, 1].plot(history.history['loss'], label='Training Loss')\n",
    "    axes[0, 1].plot(history.history['val_loss'], label='Validation Loss')\n",
    "    axes[0, 1].set_title(f'{title} - Loss')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # Precision\n",
    "    if 'precision' in history.history:\n",
    "        axes[1, 0].plot(history.history['precision'], label='Training Precision')\n",
    "        axes[1, 0].plot(history.history['val_precision'], label='Validation Precision')\n",
    "        axes[1, 0].set_title(f'{title} - Precision')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Precision')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True)\n",
    "    \n",
    "    # Recall\n",
    "    if 'recall' in history.history:\n",
    "        axes[1, 1].plot(history.history['recall'], label='Training Recall')\n",
    "        axes[1, 1].plot(history.history['val_recall'], label='Validation Recall')\n",
    "        axes[1, 1].set_title(f'{title} - Recall')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Recall')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot all training histories\n",
    "plot_training_history(history_ae, 'Autoencoder')\n",
    "plot_training_history(history_phase2, 'ResNet50 Classifier')\n",
    "plot_training_history(history_hybrid, 'Hybrid Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Step 9: Download Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ZIP file with all trained models\n",
    "def create_models_zip():\n",
    "    zip_filename = 'trained_pneumonia_models_gdrive.zip'\n",
    "    \n",
    "    with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "        # Add all model files\n",
    "        model_files = [\n",
    "            config.AUTOENCODER_PATH,\n",
    "            encoder_path,\n",
    "            config.CLASSIFIER_PATH,\n",
    "            config.HYBRID_MODEL_PATH\n",
    "        ]\n",
    "        \n",
    "        for model_file in model_files:\n",
    "            if os.path.exists(model_file):\n",
    "                zipf.write(model_file, os.path.basename(model_file))\n",
    "                file_size = os.path.getsize(model_file) / (1024 * 1024)  # MB\n",
    "                print(f\"‚úÖ Added {os.path.basename(model_file)} ({file_size:.1f} MB) to ZIP\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Model file not found: {model_file}\")\n",
    "    \n",
    "    return zip_filename\n",
    "\n",
    "# Create and download models ZIP\n",
    "models_zip = create_models_zip()\n",
    "print(f\"\\nüì¶ Models packaged in: {models_zip}\")\n",
    "\n",
    "# Show file sizes\n",
    "if os.path.exists(models_zip):\n",
    "    zip_size = os.path.getsize(models_zip) / (1024 * 1024)  # Convert to MB\n",
    "    print(f\"ZIP file size: {zip_size:.1f} MB\")\n",
    "\n",
    "# Download the ZIP file\n",
    "files.download(models_zip)\n",
    "print(\"‚úÖ Models downloaded to your computer!\")\n",
    "\n",
    "print(f\"\\nüíæ Models are also backed up in your Google Drive at:\")\n",
    "print(f\"   {gdrive_models_dir}\")\n",
    "print(\"\\nüìã Next steps:\")\n",
    "print(\"1. Extract the ZIP file to your local 'models' directory\")\n",
    "print(\"2. Use colab_local_inference.py for predictions on your laptop\")\n",
    "print(\"3. Example: python colab_local_inference.py --image path/to/xray.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 10: Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the hybrid model with sample images\n",
    "def test_prediction(model, test_generator, num_samples=5):\n",
    "    # Get a batch of test images\n",
    "    test_generator.reset()\n",
    "    x_batch, y_batch = next(test_generator)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(x_batch[:num_samples])\n",
    "    \n",
    "    # Display results\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(20, 4))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Display image\n",
    "        axes[i].imshow(x_batch[i])\n",
    "        axes[i].axis('off')\n",
    "        \n",
    "        # Get prediction\n",
    "        pred_class = np.argmax(predictions[i])\n",
    "        pred_confidence = predictions[i][pred_class]\n",
    "        true_class = np.argmax(y_batch[i])\n",
    "        \n",
    "        pred_label = config.CLASSES[pred_class]\n",
    "        true_label = config.CLASSES[true_class]\n",
    "        \n",
    "        # Set title with prediction\n",
    "        color = 'green' if pred_class == true_class else 'red'\n",
    "        axes[i].set_title(f'Pred: {pred_label}\\nTrue: {true_label}\\nConf: {pred_confidence:.2%}', \n",
    "                         color=color, fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Test the hybrid model\n",
    "print(\"üîç Testing Hybrid Model Predictions:\")\n",
    "test_prediction(hybrid_model, test_gen)\n",
    "\n",
    "print(\"\\nüéâ TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(\"üìä Training Summary:\")\n",
    "print(f\"- Autoencoder training time: {ae_time/60:.1f} minutes\")\n",
    "print(f\"- Classifier training time: {classifier_time/60:.1f} minutes\")\n",
    "print(f\"- Hybrid training time: {hybrid_time/60:.1f} minutes\")\n",
    "print(f\"- Total training time: {(ae_time + classifier_time + hybrid_time)/60:.1f} minutes\")\n",
    "print(f\"\\nüíæ Models saved locally and backed up to Google Drive\")\n",
    "print(f\"üì• Models downloaded to your computer\")\n",
    "print(\"\\n‚úÖ Ready for local inference on your laptop!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "python3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}